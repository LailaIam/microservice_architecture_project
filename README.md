# Проект microservice_architecture

## Введение
Микросервисная архитектура в машинном обучении решает несколько ключевых задач:
- изолированное развертывание отдельных компонентов (генерация данных, инференс модели, метрики, визуализация);
- независимые зависимости и версии библиотек для каждого сервиса;
- масштабирование по частям (например, увеличивать только мощность инференса);
- устойчивость к сбоям: падение одного сервиса не блокирует всю систему, если есть очереди сообщений.
  
В учебном проекте мы симулируем потоковую ML-систему с обменом данными через RabbitMQ и оркестрацией контейнеров через Docker Compose. Структура проекта следует реальным практикам: каждый сервис — в отдельной папке, со своим Dockerfile, собственной логикой и точкой входа.

В качестве исходного набора данных был взят датасет diabetes.

Ключевая идея: слабая связность сервисов достигается благодаря брокеру сообщений (RabbitMQ) и договоренному форматированию сообщений (очереди, ключи маршрутизации, идентификаторы).

## Архитектура модели

### Сервисы и их назначения
- Features (генератор данных): выбирает случайное наблюдение из датасета, формирует X (признаки) и Y_true (истинное значение таргета), присваивает уникальный ID и публикует сообщения в две очереди через RabbitMQ.
- Model (инференс): подписывается на очередь с признаками (features/X), применяет модель (в учебном варианте — простая сумма признаков), публикует предсказания Y_pred в отдельную очередь.
- Metrics: подписывается на обе очереди (Y_true и Y_pred), синхронизирует сообщения по общему ID, вычисляет абсолютную ошибку и записывает ее вместе с ID, Y_true, Y_pred в CSV.
- Plot (визуализация): периодически (каждые 5 секунд) читает CSV из локальной директории logs, строит гистограмму ошибок и сохраняет PNG. Этот сервис не использует RabbitMQ, работает по файловой модели.
  
### RabbitMQ как «конвейер» обеспечивает:
- буферизацию сообщений: если потребитель временно недоступен, сообщения не теряются;
- an-out потребления: один поток (например, Y_pred) может быть полезен сразу нескольким сервисам;
- асинхронность и разобщенность: отправитель не знает и не зависит от конкретных потребителей.

## Обмен данными

### Очереди
- features/X — признаки;
- features/Y_true — истинные значения;
- predictions/Y_pred — предсказания модели.
Названия очередей должны быть явными и согласованными между сервисами.
### Идентификаторы сообщений
Каждому наблюдению присваивается уникальный ID. Он используется во всех сообщениях, связанных с этим наблюдением, и служит ключом для синхронизации в Metrics. Это критично, потому что сообщения могут приходить в разном порядке.

### Задержки и синхронизация
В учебной версии генератор данных добавляет искусственную задержку (time.sleep) для избежания «забивания» очередей сверхскоростной отправкой. В реальных системах задержки часто возникают естественно:
- инференс модели — самый «тормозной» компонент;
- сенсоры в продакшне могут давать данные с рассинхронизацией по времени (разные датчики — разные частоты, задержки).

## Отказоустойчивость модели

### Docker Compose: depends_on и restart
- depends_on задает порядок: например, Plot зависит от Metrics.
- restart: статус "always" (или "on-failure") позволяет автоматически перезапускать сервисы при падениях. Это особенно полезно, если RabbitMQ поднимается медленнее, чем потребители: их временные сбои будут «залечены» перезапусками.
  
### Внутренние retries (ретраи) в коде
Сервисы реализуют собственные попытки подключения:
- try/except при соединении с RabbitMQ;
- ожидание 2 секунды и повтор попытки;
- break при успешном соединении — выход из «вечного» цикла и переход к нормальной работе.
Двойная защита: на уровне Docker (restart) и на уровне кода (retries). Это повышает устойчивость к «гонкам» запуска и кратковременным сетевым сбоям.

### «Вечные» циклы сервисов
Сервис — это процесс, который всегда готов обработать новые данные. Поэтому паттерн while True с корректными задержками и обработкой исключений — норма для таких компонентов. Управление жизненным циклом (старт/стоп/перезапуск) выносится на уровень оркестратора (Docker).

## Работа с файлами и визуализация

### Logs/CSV как общий артефакт
Директория logs может отсутствовать при первом запуске контейнера — сервисы должны уметь создавать ее при необходимости. CSV-файл инициируется пустой или с заголовком. Metrics создает файл при отсутствии и пишет строки с полями:
- id;
- y_true;
- y_pred;
- error (абсолютная ошибка |y_true − y_pred|).
  
### Plot: безопасное чтение
- Проверяет, что CSV не пуст (кроме заголовка);
- Обрабатывает возможные ошибки парсинга (формат, типы);
- Пишет в лог состояние: файл пуст, файл не найден, график обновлен и т.д.;
- Работает с периодическим таймером (каждые 5 секунд), чтобы не зависеть от сообщений.
  
## Docker: конструирование и сборка

### Раздельные Dockerfile
Каждый сервис имеет собственный Dockerfile, включающий:
- базовый образ (например, python:3.10-slim);
- копирование зависимостей (requirements.txt) и их установка;
- копирование кода сервиса из соответствующей папки;
- команду запуска (CMD или ENTRYPOINT).

Почему отдельные Dockerfile:
- зависимости могут отличаться между сервисами (в реальной жизни — почти всегда);
- упрощенное обновление и масштабирование конкретных компонентов.
Учебный компромисс: можно тянуть общий requirements.txt из центральной папки, если зависимости одинаковы. Но это исключение, а не правило.

### Выбор базового образа: Python slim
Образы «slim» — облегченные, без редко используемых компонентов. Плюсы:
- уменьшение размера контейнера;
- экономия памяти;
- быстрее скачиваются и разворачиваются.
Минус: если нужны экзотические зависимости (например, специфические системные библиотеки), их придется явно установить.
Большая часть функционала языка/библиотек используется редко; slim-образ закрывает типовые нужды большинства сервисов.

### Docker Compose (docker-compose.yml)
Файл оркестрации описывает:
- список сервисов (features, model, metric, plot, rabbitmq);
- контексты сборки и Dockerfile для каждого;
- зависимости (depends_on);
- политики перезапуска (restart);
- сети, тома, переменные окружения при необходимости.
  
Запуск:
- docker-compose up — соберет образ при необходимости и поднимет сервисы;
- docker-compose down — остановит;
- логирование доступно через интерфейс Docker Desktop или командой docker-compose logs.
  
## Подписки, библиотеки и протоколы: роль pika
pika — это Python-библиотека для работы с RabbitMQ. 
Она обеспечивает:
- создание соединений и каналов;
- объявление очередей (идемпотентно: если очередь уже есть — не ошибка);
- публикацию и потребление сообщений;
- подтверждения доставки;
- колбэки (callback) при поступлении новых сообщений.
Без pika (или аналогов) пришлось бы организовывать общение напрямую поверх TCP/HTTP, поддерживать адресацию, устойчивость, подтверждения, что усложняет архитектуру и снижает надежность.

## Синхронизация потоков
Проблема: сообщения от разных источников приходят в разные моменты времени и могут быть «невыравнены». 

Решение в Metric:
- хранить частичные состояния по ID, пока не получены оба значения;
- как только пара собрана — вычислить метрику и сбросить результат;
- логировать ожидание сообщений и состояние очередей.
Это общий шаблон для многопоточных ML-пайплайнов, особенно в системах, где данные поступают из нескольких сенсоров или подсистем.

## Пример сценария работы системы
- Features выбирает наблюдение, присваивает ID=123, публикует X и Y_true в соответствующие очереди.
- Model читает X(ID=123), считает сумму признаков и публикует Y_pred(ID=123).
- Metric получает (возможно, в обратном порядке) Y_true(ID=123) и Y_pred(ID=123), объединяет по ID, вычисляет |Y_true − Y_pred| и пишет строку в CSV: 123, y_true, y_pred, error.
- Plot через 5 секунд читает CSV, видит новые строки, обновляет гистограмму ошибок, сохраняет PNG и пишет в лог «Histogram updated [plot]».

## Запуск модели
### Сборка и запуск
- Убедиться, что RabbitMQ объявлен в docker-compose и доступен по имени хоста (например, rabbitmq) для клиентов pika.
- Запуск: команда "docker-compose up --build" — сборка образов, поднимает контейнеры, следит за перезапусками.
- Проверка логов: docker-compose logs или через Docker Desktop.

## Итог
Эта учебная система демонстрирует полный цикл потоковой ML-обработки: генерация признаков, инференс, оценка ошибки, визуализация — все разнесено по микросервисам, общение идет через очереди, а устойчивость обеспечивается ретраями и политиками перезапуска.

